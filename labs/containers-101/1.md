# Containers 101

## Objective

**Completion time: 10 minutes**

This learning lab provides background on the term "Containers": what they are, what they aren't, and how they relate to technologies (such as Docker) that are often referred to interchangeably with the term "Containers".

## Audience

* DevOps engineers
* Application developers
* IT teams addressing the developer need for Docker and Containers

## What are Containers?

In the simplest sense, containers are just a way of isolating running processes or code without using what we know as virtual machines (VMs) or full virtualisation.

Imagine being able to guarantee that App A and App B cannot see or interfere with each other on the same host (each does not even know that the other exists) *without* having to spin up a full VM and operating system for each. Essentially, containers give us this functionality.

### How do They Work?

Containers achieve this isolation through features built into the operating system. Just as a hypervisor is responsible for creating and managing VMs, Linux now has the ability to create isolated containers.

In Linux, this technology is called **CGroups**. CGroups allow you to portion off resources from the system.

>**From Wikipedia:** [CGroups](https://en.wikipedia.org/wiki/Cgroups) (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.


The Windows OS also has similar isolation features for containerizing Windows apps and processes.

The important thing to note is that there is only one kernel, with possibly many containers, all running on one operating system that manages the isolation of the containers. In contrast, virtualization requires installing a whole operating system for each virtual machine.

### A History Lesson

This type of separation is nothing new; technologies that "containerize" (separate) applications from each other have existed for a long time:

* 1960's Mainframes supported lightweight isolation of tasks (aided by hardware).
* Solaris 10 Zones provided a very similar solution to what we class as containers today.
* `chroot` jails on UNIX/Linux are an early attempt at the same lightweight isolation.

Each solution came with varying levels of success and, more importantly, security.

## What's Changed? Why are Containers so Important Now?

The fact you're already three paragraphs into this learning lab is part of the problem.... or used to be.

Developers (or the Operations teams they relied upon) used to have to care deeply about all of the above if they wanted to use containers; for example, they needed to know the ins and outs of CGroups. Then, once they worked out how to isolate their app, they still need a way of actually installing that "container" onto their system, then running it, updating it, and so on ... the usual software deployment lifecycle.

Simply put, containers of the past were a technology, not a polished product. For most, the barrier to entry was too high, and the advantages were too unclear.

### Containers Today: a Cast of Supporting Players

Today, the word "Containers" is as much about the tools that have grown *around* these technologies as it is about containerization technologies themselves. There is increased focus on user experience, reducing the investment of "time to get stuff done" by developers and Operations staff.

While CGroups are still used under the covers, most people using containers today don't know that... they don't have to! Current tools have made the deployment lifecycle a lot easier.

The increased usability afforded by this extra tooling is driving the recent explosion in the use of containers.