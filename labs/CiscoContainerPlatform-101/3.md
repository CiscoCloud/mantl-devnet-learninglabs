# Step 3: API Cluster Creation

Time to create our second cluster! Re-Open the API docs if you closed them and re-authenticate like you did earlier in the Lab. [Link: https://10.10.20.110/2/swaggerapi/](https://10.10.20.110/2/swaggerapi/).

Click on `Expand Operations` in the endpoint item entitled `2/clusters : List of cluster endpoints`.

Scroll down to the green `POST` request, which allows us to create a new cluster.

Use the following text in the BODY field, notice that these are all the same options (potentially in UUID form) which we specified from the UI.

```
{
   "is_harbor_enabled":false,
   "provider_client_config_uuid":"5215ae97-c11a-435c-80fa-06adef135f45",
   "name":"api-cluster",
   "kubernetes_version":"1.14.8",
   "ssh_key":"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINksnIyqCWNUdHoXx3G8PVg8Z9tWnSGGI/r2F/REwh3a gitlab",
   "description":"Cluster created via API",
   "datacenter":"CCP",
   "cluster":"CCP",
   "resource_pool":"CCP/Resources",
   "networks":[
      "VMNetwork"
   ],
   "datastore":"CCPDatastore",
   "storage_class":"vsphere",
   "workers":1,
   "ssh_user":"ccpuser",
   "type":1,
   "masters":1,
   "deployer_type":"kubeadm",
   "ingress_vip_pool_id":"55c267fd-d8cd-4c52-b288-4ad2697fe57a",
   "load_balancer_ip_num":4,
   "is_istio_enabled":false,
   "registries_root_ca":[
      ""
   ],
   "aws_iam_enabled":false,
   "aws_iam_role_arn":"",
   "worker_node_pool":{
      "vcpus":2,
      "memory":16384,
      "template":"ccp-tenant-image-1.14.8-ubuntu18-5.1.0"
   },
   "master_node_pool":{
      "vcpus":2,
      "memory":16384,
      "template":"ccp-tenant-image-1.14.8-ubuntu18-5.1.0"
   },
   "node_ip_pool_id":"55c267fd-d8cd-4c52-b288-4ad2697fe57a",
   "network_plugin":{
      "name":"calico",
      "status":"",
      "details":"{\"pod_cidr\":\"192.168.0.0/16\"}"
   },
   "deployer":{
      "proxy_cmd":"StrictHostKeyChecking no\nHost 15.29.3?.* !15.29.30.* !15.29.31.*\n ProxyCommand nc --proxy 10.193.231.10:8111 --proxy-type socks4 %h %p",
      "provider_type":"vsphere",
      "provider":{
         "vsphere_datacenter":"CCP",
         "vsphere_datastore":"CCPDatastore",
         "vsphere_client_config_uuid":"5215ae97-c11a-435c-80fa-06adef135f45",
         "vsphere_working_dir":"/CCP/vm"
      }
   }
}
```
It will create us a one worker node cluster called "API-Cluster".

Click the `Try it out!` button.

The API call will currently block until the cluster is provisioned (then return a 200 if successful!) So in the meantime....

Change back to the Cisco Cloud Platform UI, notice a new cluster soon appears as `Status: PROVISIONING`, just as with the UI creation.

![](assets/images/api-cluster-provisioning.png)

Congratulations! You just used the CCP API to spin up a new Kubernetes cluster!

This lets us easily tie the entire lifecycle of the cluster into CI/CD deployment if necessary.

## Automating cluster access
Continuing the automated, "Kubernetes as a Service" theme; these credentials and URL's for accessing the cluster are also available via the Cisco Container Platform API.

If you still have the Swagger UI open, you will see an API call labelled `/2/clusters/{clusterUUID}/dashboard`.
Providing the UUID of your cluster to this `GET` request will return the URL of the Kubernetes dashboard.

Similarly, `/2/clusters/{clusterUUID}/env` will provide you with the authentication file, which could be used by CI/CD to then automatically deploy workloads onto the new cluster!

  ![](assets/images/swagger_get_cluster_dashboard.png)

## Deploying an APP on our cluster!

Demo apps are fun, real apps are better!

We're going to use Kubernetes to deploy an installation of "OwnCloud", an open source file synchronization and sharing web-app, similar to Dropbox, Google Drive, etc.

The deployment is made up of the following Kubernetes objects:
- Deployment / Pods:         The OwnCloud application itself including a mariaDB database.
- Service / LoadBalancer:    Our real-world IP that will allow us to access the application from our enterprise.
- Persistent Volume Claims:  Cisco Container Platform has pre-configured Kubernetes to use HyperFlex storage, giving Kubernetes apps easy access to persistent storage for databases or other storage requirements.

The Kubernetes deployment manifest for our OwnCloud application is here:
[copy the whole JSON file from the page to the clipboard]

```
kind: Service
apiVersion: v1
metadata:
  name: owncloud
  labels:
    app: owncloud
spec:
  selector:
    app: owncloud
  ports:
    - protocol: "TCP"
      port: 80
      targetPort: 80
  type: LoadBalancer
  loadBalancerIP: 10.10.20.121
---
apiVersion: v1
kind: Secret
metadata:
  name: owncloud
  labels:
    app: owncloud
type: Opaque
data:
  # Create these password by running
  # echo -n "your_password_here" | base64
  # and pasting the output in here
  # demopassword
  mariadb_root_password: ZGVtb3Bhc3N3b3Jk
  owncloud_admin_password: ZGVtb3Bhc3N3b3Jk
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata: 
  name: owncloud-config-claim
  labels:
    app: owncloud
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "1Gi"
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata: 
  name: owncloud-data-claim
  labels:
    app: owncloud
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "5Gi"
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata: 
  name: owncloud-apps-claim
  labels:
    app: owncloud
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "2Gi"
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mariadb-owncloud-claim
  labels:
    app: owncloud
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "10Gi"
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: owncloud-configmap
  labels:
    app: owncloud
data:
  owc_db_type: mysql
  owc_db_name: owncloud
  owc_db_root_user: root
  owc_admin_user: demo
  # NB: This is the ip address, or name, of a domain that ownCloud will allow
  # users to log in to the web interface from. Once the application is
  # running additional domains can be added either via the web interface or 
  # 'php occ' tool
  owc_trusted_ip: 10.10.20.121
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: owncloud-dockerhub
  labels:
    app: owncloud  
spec:
  replicas: 1 
  template:
    metadata:
      # unlike a "name" is not included, instead it is automatically
      # generated based upon the deployment name.
      labels:
        app: owncloud
        # track: stable
      # annotations:
    spec:
      volumes:
      - name: config-store
        persistentVolumeClaim:
          claimName: owncloud-config-claim
      - name: data-store
        persistentVolumeClaim:
          claimName: owncloud-data-claim
      - name: app-store
        persistentVolumeClaim:
          claimName: owncloud-apps-claim
      - name: mdata-store
        persistentVolumeClaim:
          claimName: mariadb-owncloud-claim

      containers:
      - name: mariadb
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: owncloud
              key: mariadb_root_password        
        image: mariadb:10.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mdata-store
          mountPath: /var/lib/mysql
          
      - name: owncloud
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: owncloud
              key: mariadb_root_password
        - name: OWC_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: owncloud
              key: owncloud_admin_password
        - name: OWC_DB_TYPE
          valueFrom:
            configMapKeyRef:
              name: owncloud-configmap
              key: owc_db_type
        - name: OWC_DB_NAME
          valueFrom:
            configMapKeyRef:
              name: owncloud-configmap
              key: owc_db_name
        - name: OWC_DB_ROOT_USER
          valueFrom:
            configMapKeyRef:
              name: owncloud-configmap
              key: owc_db_root_user
        - name: OWC_ADMIN_USER
          valueFrom:
            configMapKeyRef:
              name: owncloud-configmap
              key: owc_admin_user
        - name: OWC_TRUSTED_IP
          valueFrom:
            configMapKeyRef:
              name: owncloud-configmap
              key: owc_trusted_ip
            
        image: owncloud:9.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80

        volumeMounts:
        - name: config-store
          mountPath: /var/www/html/config
        - name: data-store
          mountPath: /var/www/html/data
        - name: app-store
          mountPath: /var/www/html/apps

        command:
        - bash
        - "-c"
        - |
          # Enable bash debug tracing and blow up if any command fails
          set -ex
          # In order to get this running successfully in k8's what we'll do is essentially
          # copy the normal Docker startup process. Which is essentially to untar owncloud 
          # into a apache install and then fire up apache. But instead of the untar and then 
          # immediately firing up apache, we'll inject the necessary setup to get things 
          # playing nicely in k8's.
          # Extracted from the Owncloud Docker image with a docker inspect owncloud:9.1
          OWC_DOCKER_WORKING_DIR=/var/www/html
          OWC_DOCKER_ENTRYPOINT=/entrypoint.sh
          OWC_DOCKER_CMD=apache2-foreground
          # Some of the files need to have specific perms and some of the commands
          # are best executed as the www user in order that things work nicely
          OWC_DOCKER_USER=www-data
          OWC_DOCKER_GROUP=nogroup
          # No sudo installed, so need to emulate
          SU_CMD_STEM="su --login www-data --shell /bin/sh --command "
          # Perform default Owncloud Docker intialisation because we've overridden it 
          # and we need it in in place before we can do a command line installation.
          # (this does untar etc on the owncloud install)
          cd $OWC_DOCKER_WORKING_DIR
          $OWC_DOCKER_ENTRYPOINT
          # Check we have got what looks like a working owncloud install after running the entrypoint
          $SU_CMD_STEM "which php"
          ls occ
          # Now we tailor the installation to run under kubernetes. Some of this we only want to 
          # do once, as the Owncloud will tailor the settings itself and we do not want to 
          # stamp on those. Others we need to do every time because they are specific to the 
          # pod instance.
          
          RUN_BEFORE_FLAG_FILE=config/k8initialised.touch
          num_trusted_domains=0
          if [[ -f  $RUN_BEFORE_FLAG_FILE ]]; then
            # Then the disk has already been set up.
            echo INFO Owncloud initialised on this volume, not running config setup again.
          else
            echo INFO Initialising Owncloud on this volume.
            # Fix broken ownership from the install on data (comes out as root otherwise)
            chown --recursive $OWC_DOCKER_USER:$OWC_DOCKER_GROUP data 
            # Invoke Owncloud's command line installation tools to add DB, trusted_domain etc.
            $SU_CMD_STEM "cd $OWC_DOCKER_WORKING_DIR && php occ maintenance:install --database $OWC_DB_TYPE --database-host 127.0.0.1 --database-name $OWC_DB_NAME  --database-user $OWC_DB_ROOT_USER --database-pass $MYSQL_ROOT_PASSWORD  --admin-user $OWC_ADMIN_USER --admin-pass $OWC_ADMIN_PASSWORD"
            
            # Add a file so that we do not run the same set up again and stamp
            # on the existing setup
            touch   $RUN_BEFORE_FLAG_FILE
          fi
          # Add trusted_domains so that can access with Owncloud
          $SU_CMD_STEM "cd $OWC_DOCKER_WORKING_DIR &&  php occ config:system:set trusted_domains $num_trusted_domains --value=$(OWC_TRUSTED_IP)"
          num_trusted_domains=`expr $num_trusted_domains + 1`
          #Fire off the normal Docker startup
          $OWC_DOCKER_CMD
```


For this deployment to work, since we've hardcoded the "public" IP we want in the manifest, open and authenticate to the `Sandbox-Demo-Cluster-1` Kubernetes Dashboard.

Using the Kubernetes Dashboard, we can now deploy this App, click on `Create` at the top right of the Dashboard UI:
  ![](assets/images/dashboard-create.png)

In the blank field that opens, paste the entire JSON document we just copied then click `Upload`.
![](assets/images/dashboard-json.png)

(This is exactly like deploying Kubernetes applications and services from the `kubectl` command with `kubectl create -f <file.json>`, only doing it via the UI saves us from having to configure the desktop's local Kubernetes CLI.)

## Congratulations! You've created two clusters and deployed working applications!
Kubernetes was already an extremely powerful tool for fast, easy deployment and management of containerized applications. Now with Cisco Container Platform, the complexity and staffing requirements of building out and maintaining the Kubernetes infrastructure itself has been replaced by a provisioning API.

Click `Next` for the final part of this Lab; testing our newly deployed OwnCloud service!
