# What are Microservices?

The mircoservice-based approach to architecting an application uses an API to expose discrete functions of the application; these functions are known as *microservices*.  While microservices applications frequently use with containers, a microservices application does not have to be based on container technology.  In many ways, the microservice approach is more of a mentality than a specific technology decision.  Having said that, it's quite likely that if you take a microservice-based approach to architecting your application, you may decide to use containers, as well.

# What is a Container?

Containers have a long legacy in computing that stretches back to the mid-1970s.  Recently, however, Docker has popularized containers by making them really easy to use.  

There is some confusion about containers being something like virtualization. Although this generalization is reasonable at a high level, there are a few important characteristics that separate containers from virtualization.

* Containers may run on a bare-metal system or in a virtualized OS.
* Containers (at least the Docker model) are "immutable," meaning that they cannot be changed.  In fact, they are versioned, similar to source control.
* An application in a container typically runs as `root`. However, it does not have unrestricted access to the entire system on which it runs.

A container allows the developer to package and run his or her own "user-space" within an OS, separate from the rest of the system. Your code, library dependencies, runtime, frameworks, and more are versioned into the container and then run wherever you specify.  

Unlike virtualization, however, there is still only one actual operating system and kernel beneath the container. The kernel is responsible for creating all of the separate container runtime environments on top of the host kernel and OS.

Containers are very similar to the old UNIX/Linux concept of a "`chroot` jail", with two notable improvements:

* Containers provide kernel capabilities to enforce the separation of processes/file/network access. Linus refers to this feature as `CGroups`.
* Docker provides a standard for creating, packaging and redistributing the contents of the "`chroot` jail" as an image that is easy to distribute across your infrastructure or with other people. Thus, the use of Docker makes it very simple to ensure that all instances of your application are running in the same way with the same setup and code.

# How Microservices and Containers Help

The primary benefit of a microservices approach is that it enables a team to deploy application logic in a granular way.  For example, consider a scenario in which you have a User service (login, registration, etc) and a Photos service (upload photos, display photos, etc). In a traditional monolithic architecture, if you wanted to add a new feature to the Photos service, (Geo Tagging, for example), you would need to edit, rebuild, re-test and re-deploy the entire application.  In a microservices architecture, you would make updates to your Photos service, and deploy that service only. The User service would remain untouched.  

If you use containers as part of your microservices architecture, you also reap the benefit that all of your dependencies for the Photos services are "embedded" within the container.  A subtle, but important, benefit is that you can then use different languages for different parts of your application.  Not only does this separation help you to optimize based on technology superiority for a given task, it also enables you to have team members who are specialists in different languages, frameworks and stacks.

## Microservices and Containers Without Automation: a Good Way to Waste Time.
There are huge time savings to be had when considering a new application designed using microservice architecture and containers; to name a few:

* Your team has to focus only on their service, reducing information overload on large projects and enabling each service team to develop at a quicker rate.
* Each component becomes much less complex. Because interfaces to other components and services must be defined clearly, your API documentation practically writes itself.
* Time to reproduce environments is reduced, as the container images are already portable.
* Service teams can use the language known to them, potentially saving the time required to learn a new language in order to fit into the chosen language of a monolithic approach.

However, there is the potential that a microservices-based approach can create far more artifacts to manage and deploy to production than a monolithic application would create, as each service will produce its own container image. Building, versioning and deploying these yourself would cut into the time savings created and it introduces new potential for human error along the way.

## Automation: a Better Way
Containers and Microservices are perfect candidates for automation throughout the whole application lifecycle. Typically, microservices-based development works as follows:

* **Development:** When source code changes, code tests are run, service is packaged as a new container, and integration tests are run using the new version of the services container. This is known as **Continuous Integration (CI)**.

* **Deployment:** If a new version of a service passes required testing, it can be passed to a container stack automatically (more later), triggering an  automatic rolling upgrade of the new version into production. This is known as **Continuous Deployment (CD)**.

There is wide scope in terms of the depth and tool choices made by a given development team for a given project, all of which falls under the umbrella terms of **CI** and **CD** for automating the code-to-production lifecycle.

# Managing a Plethora of Containers

Once you've decided that you want to at least experiment with a microservices architecture, you will quickly find a vast array of platforms on which you can deploy your containers in the public and private cloud world.  

If you are considering a private cloud model, you will probably come across some container platforms: Kubernetes, Docker Swarm, Mesos, and Marathon being the most common. Companies also offer hosted versions of the same stacks (such as Kubernetes with Google's *Google Container Engine* or GKE).

Each of these technologies provides an automated way to manage running your containers and services. They include APIs and user tools that allow you (or your CI/CD automation) to deploy your containers automatically across a pool of compute resources and subsequently to perform common lifecycle tasks against the deployment; for example:

* Blue/Green Upgrades (Zero downtime upgrades of a microservices application).
* Access to application and service logs without requiring developers to log onto production machines. (Self serve application logs).
* Scaling and Load balancing multiple instances of an application to handle increased service load and resilience.

These so-called "container stacks" enable you to take an application that's been packaged as a container and launch it into a cluster (such as Kubernetes), which will take care of the scaling, service health and service resilience (restarting failed services).


# Open Source or Proprietary?
Public cloud providers may also have their own container platforms, such as the *EC2 Container Service* that Amazon AWS offers. These can be very convenient for development teams who don't have their own physical or hosted infrastructure. However, these container stacks are sometimes proprietary, meaning that you may not be able to use the same automation with both your public and private/Local container deployments.

For example, you can run Kubernetes (or Mesos and Marathon) anywhere that you have spare compute resources (VM's, Bare metal, Amazon EC2, Microsoft Azure, Openstack, and so on). This approach gives your development team a consistent API for automating deployments at *all* of your locations.  

In comparison, automating your container deployments against the AWS Container Service, for example, will mean must use AWS resources for all of your deployments. (All of them must be within AWS because the AWS API does not allow the use of non-AWS resources.)

Setting up a secondary location outside of AWS would require a deployment of something completely different (such as Kubernetes) and the management of two separate deployment automation workflows for AWS APIs and Kubernetes APIs. In this case, more is not merrier.

Thus, EC2 VM resources with Kubernetes on top (as referenced above) allows leverage of AWS compute resources while preventing a "lock-in" to AWS-only resources. This architectural decision may be a significant factor if you plan to scale your deployments to hybrid or multi-cloud providers.

# Conclusion

Although we've only scratched the surface of the new era of cloud-native application development, you should now have a better understanding of cloud application architecture, and an awareness of some of the considerations to keep in mind when exploring the world of microservices and containers.


# Want to Deploy Your First Containerized Application?

Progress to our **Docker-101** and **Kubernetes-101** Learning Labs to build, deploy and test your own Docker-packaged application, and to take your understanding of containers and microservices to the next level.
