# What are microservices?

Microservices is an approach to architecting an application.  Generally microservices are thought of as discrete functions in your application exposed by an API.  While microservices and containers are used a lot together, you don't necessarily have to have a microservices application based on container technology.  In many ways, microservices is more of a mentality, than a specific technology decision.  Having said that, it's quite likely that if you architect in a microservices manner, it may lead to using containers.

# What is a container?

Containers have a long legacy in computing that stretch back to the mid-1970s.  However, recently Docker has come to the fore and popularized containers by making them really easy to use.  

There is some confusion about containers being something like virtualization, which at a high level generalization is reasonable.  The major differences come in a few characteristics that seperate containers from virtualization.

* Containers may live on bare metal or in a virtualized OS
* Containers, at least the Docker model, are "immutable," meaning that they cannot be changed.  In fact, they are versioned, similar to source control.
* Applications in containers typically run as `root` (But that doesn't mean they have access to the whole system they are running on).

Containers allow developers to package and run their own "user-space" of an OS, separate from the rest of the system;
Your code, library dependancies, runtime, frameworks etc will all be versioned into the container and then run wherever you want.  

However, unlike virtualization, there is still only one actual "operating system / kernel" underneath. The kernel is responsible for creating all of the separate container runtime environments on top.

If you're familiar with the old Unix/Linix concept of chroot jails, Containers are very similar, with two notable improvements:

* There are now kernel capabilities to enforce the separation of processes/file/network access. Referred to in Linux as 'CGroups'.
* Docker provides a standard for creating / packaging and redistributing what is inside the 'chroot jail' into an image which is easy to distribute across your infrastructure or with other people, making it very simple to ensure that all instances of your application are running in the same way with the same setup and code.

# How microservices and containers help

The primary benefit of a microservices approach is they enable a team to deploy application logic in a granular way.  Take an example scenario where I have a User service (login, registration, etc) and a Photos service (upload photos, display photos, etc). In a traditional monolithic architecture, If I wanted to add a new feature to the Photos service, perhaps adding Geo Tagging, then I would need to edit, re-test and re-deploy the entire application.  In a microservices architecture, I would make updates to my Photos service, and just deploy that service. The user service would remain untouched.  

If you use containers as part of your microservices architecture, you also reap the benefit that all of your dependencies for the Photos services are "embedded" within the container.  A subtle benefit, although important, is that you can then use different languages for different parts of your application.  This can help optimize based not only on technology superiority for a given task, but also enables you to have team members who are specialists in different languages, frameworks and stacks.

# microservices and containers without automation; a good way to waste a lot of time.
There are huge time savings to be had when considering a new application designed using microservice architecture and containers, to name a few;

* Your team only has to focus on their service, reducing information overload on large projects and keeping each service team developing at a quicker rate.
* Each component becomes a lot less complex, as interfaces to other components and services must be clearly defined. Your developer documentation practically writes itself.
* Time to re-produce environments is reduced, as the container images are already portable.
* Service teams can use the language known to them, potentially saving re-skill time to fit into the monolithic approaches' chosen language.

However, we potentially end up with more artefacts to manage and deploy to production versus a monolithic application, as each service will produce it's own container image. Building, versioning and deploying these yourself would cut into the time savings created and introduce new potential for human error along the way.

## Automation: A better way.
Containers and Microservices are perfect candidates for automation throughout the whole application lifecycle. This is commonly how development looks:

* Development: When source code changes, code tests are run, service is packaged as a new container, integration tests are run using the new version of the services container. This is commonly known as CI or Continuous Integration.

* Deployment: If a new version of a service passes required testing, it can be automatically passed to a container stack (more later), triggering an  automatic rolling upgrade of the new version into production. This is commonly known as CD or Continuous Deployment.

There is wide scope in terms of the depth and tool choices made by a given development team for a given project, all covered under the umbrella terms of CI and CD for automating the code-to-production lifecycle.

# Managing a plethora of containers

Once you've decided that you want to at least experiment with a microservices architecture, you will quickly find a vast array of platforms on which you can deploy your containers in the public and private cloud world.  

If you are considering a private cloud model, you will probably come across some container platforms: Kubernetes, Docker Swarm, Mesos & Marathon being the most common. Companies also offer hosted versions of the same stacks (such as Kubernetes with Googles' "Google Container Engine" or GKE).

Each of these technologies provides an automated way to manage running your containers and services. They include API's and user tools that allow you (or your CI/CD automation) to deploy your containers automatically across a pool of compute resources and then perform common lifecycle tasks against the deployment, such as:

* Blue / Green Upgrades (Zero downtime upgrades of a microservices application).
* Access to application and service logs without requiring developers to log onto production machines. (Self serve application logs).
* Scaling - Load balancing multiple instances of an application to handle increased service load and resilience.

These so-called "container stacks" enable you to take an application that's been packaged as a container, and launch it into a cluster, such as Kubernetes, which will take care of the scaling, service health and making sure it's running.


## Open Source vs proprietary.
Public cloud providers may also have their own container platforms (such as Amazon AWS with their "EC2 Container Service"). Which can be very convenient for development teams without their own infrastructure (Physical or hosted).
However, these container stacks are sometimes proprietary, meaning you wouldn't be able to use the same automation against both your Public and Private/Local container deployments;

For example, you can run Kubernetes (or Mesos & Marathon) anywhere you have spare compute (VM's, Bare metal, Amazon EC2, Microsoft Azure, Openstack etc). This will give your development team a consistent API to automate your deployments against at *all* your locations.  

In comparison, automating your container deployments against AWS' Container Service, for example, will mean you are locked into using AWS resources for all your deployments. (they all have to be within AWS because thats all the AWS API allows).

Setting up a secondary location outside of AWS would require a deployment of something completley different (such as Kubernetes) and then you are managing deployment automation for both AWS API's and Kubernetes API's. More in this case is not merrier.

That said, as referenced above, EC2 VM resources with Kubernetes on top, allows leverage of AWS compute resources while preventing this lock in, which may be a considerable factor if you are looking to scale your deployments to hybrid or multi-cloud providers.

# Conclusion

We've only scratched the surface of the new era of Cloud Native application development.  You should now have a better understanding of cloud application architecture, and some of the considerations to keep in mind when exploring the new world of microservices and containers.


# Want to deploy your first containerized application?

Progress to our Docker-101 and Kubernetes-101 learning labs to build, deploy and test your own docker-packaged application and take your understanding of containers and microservices to the next level.
